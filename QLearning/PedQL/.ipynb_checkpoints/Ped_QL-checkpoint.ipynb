{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Q learning On The Inverted Pendulum Problem. \n",
    "# Reference: https://github.tamu.edu/desik-rengarajan/IRL\n",
    "\n",
    "from ped_car import PedestrianEnv\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "## Initialize the \"Pedestrian\" environment\n",
    "env = PedestrianEnv()\n",
    "# observation_space_low = [0,0,1.5,10,-3]\n",
    "# observation_space_high = [10,75,7.5,15,3]\n",
    "observation_space_low = [0,0,2.5,10,-3]\n",
    "# CHANGE HERE\n",
    "observation_space_high = [10,60,10,15,3]\n",
    "\n",
    "## Defining the environment related constants\n",
    "\n",
    "# Number of discrete states and actions (bucket) per dimension\n",
    "# CHANGE HERE\n",
    "NUM_BUCKETS = (21,61,4,21,13)  # (p_y, c_x, c_y, c_v, c_a) = (40, 300, 3, 40, 24) add 1 to all!\n",
    "NUM_ACTIONS = 5\n",
    "\n",
    "# bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(observation_space_low, observation_space_high))\n",
    "\n",
    "# bounds for action and state spaces\n",
    "action_space_low = -2\n",
    "action_space_high = 2\n",
    "action_bins = np.squeeze(np.linspace(action_space_low, action_space_high, NUM_ACTIONS))\n",
    "state_bins = []\n",
    "# CHANGE HERE\n",
    "for i in range(5):\n",
    "    state_bins.append(np.linspace(STATE_BOUNDS[i][0], STATE_BOUNDS[i][1], NUM_BUCKETS[i]-1))\n",
    "\n",
    "## Defining the simulation related constants\n",
    "NUM_EPISODES =300000\n",
    "# MAX_T = 200\n",
    "DEBUG_MODE = False\n",
    "\n",
    "## Creating a Q-Table for each state-action pair\n",
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "\n",
    "# with open('ped_try.pickle', 'rb') as f:\n",
    "#     q_table = pickle.load(f)\n",
    "    \n",
    "def select_action(state, explore_rate):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "        action = map_action(action)\n",
    "        \n",
    "    return action\n",
    "\n",
    "def map_action(action_idx):\n",
    "    if(action_idx == 0):\n",
    "        lower_limit = action_space_low\n",
    "        upper_limit = action_space_low\n",
    "    elif(action_idx == NUM_ACTIONS):\n",
    "        lower_limit = action_space_high\n",
    "        upper_limit = action_space_high\n",
    "    else:\n",
    "        lower_limit = action_bins[action_idx-1]\n",
    "        upper_limit = action_bins[action_idx]\n",
    "    \n",
    "    action = np.random.uniform(low=lower_limit, high=upper_limit, size=1)\n",
    "    return action\n",
    "\n",
    "def bucket_action(action):\n",
    "    return np.digitize(action, action_bins, right=True) #right is different\n",
    "\n",
    "def bucket_state(states):\n",
    "    idx = []\n",
    "    for i, state in enumerate(states):\n",
    "        idx.append(np.digitize(state, state_bins[i], right=True)) #right is different\n",
    "    return tuple(idx)\n",
    "## Instantiating the learning related parameters\n",
    "learning_rate = 1\n",
    "explore_rate = 1\n",
    "decay_rate_exp = 0.00002\n",
    "decay_rate_lea = 0.0002\n",
    "\n",
    "max_explore_rate = 1\n",
    "max_learn_rate = 1\n",
    "min_explore_rate = 0.01\n",
    "min_learn_rate = 0.01\n",
    "\n",
    "discount_factor = 0.99\n",
    "rew = np.zeros(NUM_EPISODES)\n",
    "Rate_explore = np.zeros((NUM_EPISODES,1))\n",
    "Rate_learn = np.zeros((NUM_EPISODES,1))\n",
    "\n",
    "static_count = 0\n",
    "static_death_toll=0\n",
    "static_safe_chicken=0\n",
    "\n",
    "death_toll=0\n",
    "safe_chicken=0\n",
    "done_count=0\n",
    "count=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # ## Defining the simulation related constants\n",
    "# NUM_EPISODES = 100000\n",
    "\n",
    "# import ped_car\n",
    "# reload(ped_car)\n",
    "# from ped_car import PedestrianEnv\n",
    "\n",
    "# The initial state\n",
    "for episode in range(NUM_EPISODES):\n",
    "    c_state,static_state = env.reset(np.random.randint(1,4))\n",
    "    c_state = bucket_state(c_state)\n",
    "    cum_rew_ep = 0\n",
    "    if static_state:\n",
    "        static_count+=1\n",
    "    while True:\n",
    "        # Select an action\n",
    "        action = select_action(c_state, explore_rate)\n",
    "#         print(action)\n",
    "        # Execute the action\n",
    "        n_state, reward, done = env.step(action)\n",
    "        \n",
    "#         the variable temp is for printing purpose\n",
    "        temp = n_state\n",
    "        n_state = np.array(n_state)\n",
    "#         print(reward)\n",
    "\n",
    "        # bucket states and action\n",
    "        action = bucket_action(action)\n",
    "#         print(action,\"C\")\n",
    "        n_state = bucket_state(n_state)\n",
    "\n",
    "        # Update the Q based on the result\n",
    "        best_q = np.amax(q_table[n_state])\n",
    "        try:\n",
    "            q_table[c_state + (action,)] = ((1-learning_rate) * q_table[c_state + (action,)] + \n",
    "                                        learning_rate * (reward + discount_factor * best_q))\n",
    "        except:\n",
    "            print()\n",
    "            \n",
    "        # Setting up for the next iteration\n",
    "        c_state = n_state\n",
    "        cum_rew_ep += reward\n",
    "\n",
    "        # Print data\n",
    "        if done:\n",
    "            print(temp)\n",
    "            print('EPISODE:'+str(episode)+' STATIC: '+str(static_state)\n",
    "                  +'   REWARD: '+str(reward) + ' Cumulative Reward: '+str(cum_rew_ep)) \n",
    "            \n",
    "            done_count+=1\n",
    "            if (reward==-100):\n",
    "                death_toll+=1   \n",
    "                if static_state:\n",
    "                    static_death_toll+=1\n",
    "                    \n",
    "            if (reward==75):\n",
    "                safe_chicken+=1\n",
    "                if static_state:\n",
    "                    static_safe_chicken+=1\n",
    "            break\n",
    "        \n",
    "    Rate_explore[episode] = explore_rate\n",
    "    Rate_learn[episode] = learning_rate\n",
    "\n",
    "    # Update parameters\n",
    "    rew[episode] = cum_rew_ep\n",
    "\n",
    "    #Exponential learning\n",
    "    decay_parameter_exp = np.exp(-decay_rate_exp * (episode+1))\n",
    "    decay_parameter_lea = np.exp(-decay_rate_lea * (episode+1))\n",
    "    print(explore_rate)\n",
    "    explore_rate = min_explore_rate + (max_explore_rate - min_explore_rate)*decay_parameter_exp\n",
    "    learning_rate = min_learn_rate + (max_learn_rate - min_learn_rate)*decay_parameter_lea\n",
    "\n",
    "#Results\n",
    "print('Episodes', done_count)\n",
    "print('Safe_chicken',safe_chicken)\n",
    "print('Death_toll '+str(death_toll))\n",
    "print('Did_not_reach '+str(done_count-safe_chicken-death_toll))\n",
    "print('Death_toll % '+str(death_toll*100/(done_count)))\n",
    "\n",
    "print('Static Episodes', static_count)\n",
    "print('Static Safe_chicken',static_safe_chicken)\n",
    "print('Static Death_toll '+str(static_death_toll))\n",
    "print('Static Did_not_reach '+str(static_count - static_safe_chicken - static_death_toll))\n",
    "print('Static Death_toll % '+str(static_death_toll*100/(static_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(q_table>0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(action_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100\n",
    "num_iter = NUM_EPISODES // CHUNK_SIZE\n",
    "ma_rew = np.empty((num_iter))\n",
    "for n in range(num_iter):\n",
    "    ma_rew[n] = np.mean(rew[n*CHUNK_SIZE:(n+1)*CHUNK_SIZE])\n",
    "# print(ma_rew.size)\n",
    "plt.plot(np.asarray(np.arange(ma_rew.size)), ma_rew)\n",
    "plt.xlabel('Episodes / {}'.format(CHUNK_SIZE))\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.savefig('Episodic Cumulative Reward Ped2 latest small time.pdf')\n",
    "plt.savefig('Episodic Cumulative Reward Ped2 latest small time.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.asarray(np.arange(NUM_EPISODES)),Rate_explore, label='Exploration Rate')\n",
    "plt.plot(np.asarray(np.arange(NUM_EPISODES)),Rate_learn, label='Learning Rate')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rate_learn[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(q_table)\n",
    "import pickle\n",
    "with open('ped_try.pickle', 'wb') as f:\n",
    "    pickle.dump(q_table, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.where(q_table > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
